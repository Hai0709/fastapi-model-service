import os
import asyncio
import io
import time
import torch
from PIL import Image
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
from torch.profiler import profile, ProfilerActivity


DEFAULT_MODEL_PATH = "/home/cxq/LLM_ER/fastapi_model_service/Model/qwen2.5-vl-3b-instruct"
MODEL_PATH = os.getenv("MODEL_PATH", DEFAULT_MODEL_PATH)
DEFAULT_MODEL_ID = "Qwen/Qwen2.5-VL-3B-Instruct"

torch.backends.cudnn.benchmark = True
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
def print_gpu_status(tag=""):
    if not torch.cuda.is_available():
        print("CUDA ä¸å¯ç”¨ï¼")
        return

    torch.cuda.synchronize()
    gpu_count = torch.cuda.device_count()
    print(f"\n========={tag} GPU çŠ¶æ€ ({gpu_count} å¼ å¡) =========")
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        total_mem = props.total_memory / 1024**2  # MB
        alloc_mem = torch.cuda.memory_allocated(i) / 1024**2
        peak_mem = torch.cuda.max_memory_allocated(i) / 1024**2
        percent = (alloc_mem / total_mem) * 100
        bar = "â–ˆ" * int(percent / 5) + "-" * (20 - int(percent / 5))
        print(f"GPU{i:<2} | {props.name:<25} | å½“å‰ {alloc_mem:8.1f} MB | å³°å€¼ {peak_mem:8.1f} MB "
              f"| åˆ©ç”¨ç‡ [{bar}] {percent:5.1f}%")
    print("========================================================\n")

async def _load_model():
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ï¼š{MODEL_PATH}")
    local_mode = os.path.exists(MODEL_PATH)
    model_id = MODEL_PATH if local_mode else DEFAULT_MODEL_ID

    # åŠ è½½å¤„ç†å™¨
    processor = AutoProcessor.from_pretrained(
        model_id,
        local_files_only=local_mode,
        use_fast=True,
        trust_remote_code=True,
    )

    # è‡ªé€‚åº”å¤š GPU æ˜¾å­˜åˆ†é…
    gpu_count = torch.cuda.device_count()
    max_memory = {i: "22GiB" for i in range(gpu_count)} if gpu_count > 1 else None
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        model_id,
        torch_dtype=torch.float32,
        trust_remote_code=True,
        local_files_only=local_mode,
        device_map="auto",
        max_memory=max_memory,
    ).eval()

    print("æ¨¡å‹åŠ è½½å®Œæˆï¼ˆdevice_map=auto å¤šGPUåˆ†å¸ƒï¼‰")
    print_gpu_status("æ¨¡å‹åŠ è½½å")

    # å• GPU æ‰å¯ç”¨ torch.compile
    if torch.cuda.device_count() == 1:
        try:
            model = torch.compile(model, backend="inductor", mode="max-autotune")
            print("å•å¡æ¨¡å¼å¯ç”¨ torch.compile() æå‡æ€§èƒ½")
        except Exception as e:
            print(f"torch.compile å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸæ¨¡å‹ï¼š{e}")
    else:
        print("å¤š GPU æ¨¡å¼å¯ç”¨ device_map è‡ªåŠ¨åˆ†é…")
    return processor, model

def apply_channels_last(inputs):
    for k, v in inputs.items():
        if torch.is_tensor(v) and v.ndim == 4:
            inputs[k] = v.contiguous(memory_format=torch.channels_last)
            print(f"âœ… è¾“å…¥ {k} å·²åˆ‡æ¢ä¸º channels_last æ ¼å¼")
    return inputs

async def describe_image_with_profiler(image_bytes: bytes, user_prompt: str):
    processor, model = await _load_model()

    # å›¾ç‰‡é¢„å¤„ç†
    image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
    max_side = 512
    w, h = image.size
    scale = max(w, h) / max_side
    if scale > 1:
        image = image.resize((int(w / scale), int(h / scale)))
        print(f"ğŸª„ å›¾ç‰‡ç¼©æ”¾: {w, h} â†’ {image.size}")

    # è¾“å…¥æ„é€ 
    messages = [{"role": "user", "content": [
        {"type": "image"},
        {"type": "text", "text": user_prompt}
    ]}]
    input_text = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=input_text, images=[image], return_tensors="pt")

    # æ”¾åˆ° GPU
    for k, v in inputs.items():
        if torch.is_tensor(v):
            device = list(model.hf_device_map.values())[0] if hasattr(model, "hf_device_map") else torch.device("cuda:0")
            if v.dtype in (torch.long, torch.int):
                inputs[k] = v.to(device)
            else:
                inputs[k] = v.to(device, dtype=torch.float16)
    inputs = apply_channels_last(inputs)
    print_gpu_status("è¾“å…¥å‡†å¤‡å®Œæˆ")

    # Warm-up
    print("Warm-up ä¸­ ...")
    try:
        with torch.inference_mode():
            _ = model.generate(**inputs, max_new_tokens=4, do_sample=False)
        torch.cuda.synchronize()
        print("Warm-up æˆåŠŸ")
    except Exception as e:
        print(f"Warm-up å¤±è´¥: {e}")


    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    start_evt, end_evt = torch.cuda.Event(True), torch.cuda.Event(True)
    start_evt.record()
    with torch.inference_mode():
        output = model.generate(**inputs, max_new_tokens=8, do_sample=False)
    end_evt.record()
    torch.cuda.synchronize()
    elapsed_ms = start_evt.elapsed_time(end_evt)

    print_gpu_status("æ¨ç†ç»“æŸå")
    print(f"GPU ç«¯æ¨ç†è€—æ—¶: {elapsed_ms:.2f} ms")
    decoded = processor.batch_decode(output, skip_special_tokens=True)
    print(f"æ¨¡å‹è¾“å‡º: {decoded}")

    print("å¯åŠ¨ GPU Profiler (ä»…æ•è· CUDA æ´»åŠ¨)...")
    log_dir = "./log/qwen_multi_gpu_device_map"
    os.makedirs(log_dir, exist_ok=True)

    with profile(
        activities=[ProfilerActivity.CUDA],  # ä»…çœ‹ GPU
        record_shapes=False,
        on_trace_ready=torch.profiler.tensorboard_trace_handler(log_dir)
    ) as prof:
        with torch.inference_mode():
            _ = model.generate(**inputs, max_new_tokens=8, do_sample=False)
        torch.cuda.synchronize()

    print("GPU Profiler å®Œæˆ")
    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=10))


async def main():
    test_image = os.getenv("TEST_IMAGE_PATH", "/home/cxq/LLM_ER/fastapi_model_service/app/models/test.jpg")
    if not os.path.exists(test_image):
        raise FileNotFoundError(f"æµ‹è¯•å›¾ç‰‡ä¸å­˜åœ¨: {test_image}")
    with open(test_image, "rb") as f:
        image_bytes = f.read()
    await describe_image_with_profiler(image_bytes, "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„ä¸»è¦å†…å®¹ã€‚")


if __name__ == "__main__":
    asyncio.run(main())
